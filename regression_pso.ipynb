{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "column_names = ['id','date','price','bedrooms','bathrooms','sqft_living','sqft_lot','floors','waterfront','view','condition','grade','sqft_above','sqft_basement','yr_built','yr_renovated', 'zipcode','lat','long','sqft_living15','sqft_lot15']\n",
    "\n",
    "raw_dataset = pd.read_csv('./data/kc_house_data.csv', names=column_names,\n",
    "                          na_values='?',\n",
    "                          header = 1,\n",
    "                          sep=',', skipinitialspace=False)\n",
    "raw_dataset = raw_dataset.drop(columns = ['id', 'date']) # 'lat', 'long', 'zipcode'\n",
    "dataset = raw_dataset.copy()\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('price')\n",
    "test_labels = test_features.pop('price')\n",
    "\n",
    "selected_features = selected_features = ['sqft_living', 'sqft_lot', 'bedrooms','bathrooms', 'view', 'condition', 'grade']\n",
    "# selected_features = ['sqft_living']\n",
    "#np_selected_features = np.array(train_features[selected_features])\n",
    "#normalizer = preprocessing.Normalization(input_shape=[7,])\n",
    "#normalizer.adapt(np_selected_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "def loss_one(W, X, res):\n",
    "    prediction = np.dot(W[:-1], X) + W[-1]\n",
    "    return abs(res - prediction)\n",
    "\n",
    "def loss_all(W, train_features, train_labels):\n",
    "    \"\"\"\n",
    "    :param W: weights\n",
    "    :param train_features: features\n",
    "    :param train_labels: labels\n",
    "    :return: MAE (MEAN ABSOLUTE ERROR) loss\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    for i in range(train_features.shape[0]):\n",
    "        total_loss += loss_one(W, train_features[i], train_labels[i])\n",
    "\n",
    "    return total_loss / train_features.shape[0]\n",
    "\n",
    "def loss_all_implicit(W):\n",
    "    \"\"\"\n",
    "    Wrapper that implicitely sets the train_features and train_label params.\n",
    "    Used to be passed to the PSO class bellow\n",
    "    \"\"\"\n",
    "    return loss_all(W, train_features, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "train_features = train_features[selected_features].to_numpy()\n",
    "train_labels = train_labels.to_numpy()\n",
    "test_features = test_features[selected_features].to_numpy()\n",
    "test_labels = test_labels.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = train_features.shape[1] + 1\n",
    "dims\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss one: 672224.5\n",
      "loss training: 524700.2175390399\n",
      "loss test: 517692.8043729755\n"
     ]
    }
   ],
   "source": [
    "test_run_weight = np.ones(dims)\n",
    "print(f\"loss one: {loss_one(test_run_weight, train_features[0], train_labels[0])}\")\n",
    "print(f\"loss training: {loss_all(test_run_weight, train_features, train_labels)}\")\n",
    "print(f\"loss test: {loss_all(test_run_weight, test_features, test_labels)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class PSO(object):\n",
    "    def __init__(self, func, init_pos, n_particles):\n",
    "        self.func = func\n",
    "        self.n_particles = n_particles\n",
    "        self.init_pos = np.array(init_pos)\n",
    "        self.particle_dim = len(init_pos)\n",
    "        self.particles_pos = np.random.uniform(size=(n_particles, self.particle_dim)) \\\n",
    "                             * self.init_pos\n",
    "        self.velocities = np.random.uniform(size=(n_particles, self.particle_dim))\n",
    "\n",
    "        # Initialize the best positions\n",
    "        self.g_best = init_pos\n",
    "        self.p_best = self.particles_pos\n",
    "\n",
    "    def update_position(self, x, v):\n",
    "        \"\"\"\n",
    "        x (array-like): particle current position.\n",
    "        v (array-like): particle current velocity.\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        v = np.array(v)\n",
    "        new_x = x + v\n",
    "        return new_x\n",
    "\n",
    "    def update_velocity(self, x, v, p_best, g_best, c0=0.5, c1=1.5, w=0.75):\n",
    "        \"\"\"\n",
    "        x (array-like): particle current position.\n",
    "        v (array-like): particle current velocity.\n",
    "        p_best (array-like): the best position found so far for a particle.\n",
    "        g_best (array-like): the best position regarding\n",
    "                             all the particles found so far.\n",
    "        c0 (float): the cognitive scaling constant.\n",
    "        c1 (float): the social scaling constant.\n",
    "        w (float): the inertia weight\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        v = np.array(v)\n",
    "        assert x.shape == v.shape, 'Position and velocity must have same shape'\n",
    "        # a random number between 0 and 1.\n",
    "        r = np.random.uniform()\n",
    "        p_best_np = np.array(p_best)\n",
    "        g_best_np = np.array(g_best)\n",
    "\n",
    "        new_v = w * v + c0 * r * (p_best_np - x) + c1 * r * (g_best_np - x)\n",
    "        return new_v\n",
    "\n",
    "    def optimize(self, maxiter=25):\n",
    "        losses = []\n",
    "        for it in range(maxiter):\n",
    "            for i in range(self.n_particles):\n",
    "                x = self.particles_pos[i]\n",
    "                v = self.velocities[i]\n",
    "                p_best = self.p_best[i]\n",
    "                self.velocities[i] = self.update_velocity(x, v, p_best, self.g_best)\n",
    "                self.particles_pos[i] = self.update_position(x, v)\n",
    "                # Update the best position for particle i\n",
    "                if self.func(self.particles_pos[i]) < self.func(p_best):\n",
    "                    self.p_best[i] = self.particles_pos[i]\n",
    "                # Update the best position overall\n",
    "\n",
    "                old_val = self.func(self.g_best)\n",
    "                new_val = self.func(self.particles_pos[i])\n",
    "                if new_val < old_val:\n",
    "                    self.g_best = np.copy(self.particles_pos[i])\n",
    "                    print(f\"Better loss, from {old_val} to {new_val} with particle {self.g_best}\")\n",
    "            losses.append(self.func(self.g_best))\n",
    "            print(f\"Iteration {it}, loss: {losses[-1]}, particle: {self.g_best}\")\n",
    "\n",
    "        return self.g_best, self.func(self.g_best), losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better loss, from 524700.2175390399 to 515669.7919649196 with particle [1.457 1.616 1.318 1.289 1.225 1.029 1.359 1.408]\n",
      "Better loss, from 515669.7919649196 to 509237.3617480759 with particle [1.538 2.159 1.438 1.505 1.514 1.074 1.809 1.352]\n",
      "Better loss, from 509237.3617480759 to 504440.8675858936 with particle [1.848 2.573 1.516 1.946 1.507 1.718 1.827 1.473]\n",
      "Better loss, from 504440.8675858936 to 502982.382963073 with particle [1.637 2.777 1.592 1.999 1.552 2.005 1.957 1.879]\n",
      "Better loss, from 502982.382963073 to 493426.40117611084 with particle [2.529 3.716 2.554 3.036 2.451 3.106 2.8   2.847]\n",
      "Better loss, from 493426.40117611084 to 481612.5311713106 with particle [3.439 5.249 4.085 4.581 3.529 4.403 3.892 4.475]\n",
      "Better loss, from 481612.5311713106 to 480932.050387617 with particle [3.74  5.263 4.393 4.384 3.752 4.575 4.264 4.611]\n",
      "Better loss, from 480932.050387617 to 464715.58463008306 with particle [5.405 7.973 6.192 6.436 5.558 6.55  6.2   6.797]\n",
      "Better loss, from 464715.58463008306 to 464585.0680175081 with particle [5.678 7.875 6.013 6.178 5.697 6.322 6.115 7.209]\n",
      "Better loss, from 464585.0680175081 to 460510.64399482624 with particle [6.104 8.744 6.845 6.583 6.242 6.954 6.691 8.361]\n",
      "Better loss, from 460510.64399482624 to 454903.55025108973 with particle [7.186 9.862 7.878 7.967 7.198 7.815 7.585 9.94 ]\n",
      "Better loss, from 454903.55025108973 to 443162.958089018 with particle [ 9.897 13.053 10.703 10.675  9.7   10.421  9.928 13.132]\n",
      "Better loss, from 443162.958089018 to 439520.1046077398 with particle [11.176 14.254 12.221 11.802 10.553 11.847 11.226 14.552]\n",
      "Iteration 0, loss: 439520.1046077398, particle: [11.176 14.254 12.221 11.802 10.553 11.847 11.226 14.552]\n",
      "Better loss, from 439520.1046077398 to 432281.22061283636 with particle [15.752 20.033 16.831 16.395 14.676 16.7   16.144 20.174]\n",
      "Iteration 1, loss: 432281.22061283636, particle: [15.752 20.033 16.831 16.395 14.676 16.7   16.144 20.174]\n",
      "Iteration 2, loss: 432281.22061283636, particle: [15.752 20.033 16.831 16.395 14.676 16.7   16.144 20.174]\n",
      "Iteration 3, loss: 432281.22061283636, particle: [15.752 20.033 16.831 16.395 14.676 16.7   16.144 20.174]\n",
      "Better loss, from 432281.22061283636 to 432218.15127281117 with particle [18.023 22.252 18.819 18.565 16.522 18.954 18.209 23.265]\n",
      "Better loss, from 432218.15127281117 to 430594.64831236796 with particle [16.088 18.772 17.017 16.494 14.29  16.591 15.963 21.218]\n",
      "Better loss, from 430594.64831236796 to 430011.4224732833 with particle [18.807 21.689 19.734 19.192 16.539 19.189 18.503 24.385]\n",
      "Iteration 4, loss: 430011.4224732833, particle: [18.807 21.689 19.734 19.192 16.539 19.189 18.503 24.385]\n",
      "Better loss, from 430011.4224732833 to 428875.9578290667 with particle [18.403 20.598 18.867 17.589 16.23  18.13  17.982 23.515]\n",
      "Better loss, from 428875.9578290667 to 428498.9624514869 with particle [16.95  17.33  16.862 15.41  14.434 15.978 16.067 21.752]\n",
      "Iteration 5, loss: 428498.9624514869, particle: [16.95  17.33  16.862 15.41  14.434 15.978 16.067 21.752]\n",
      "Better loss, from 428498.9624514869 to 428294.56591515976 with particle [18.637 20.438 19.212 18.202 16.343 18.802 18.13  24.088]\n",
      "Better loss, from 428294.56591515976 to 428273.2755260551 with particle [19.321 21.12  19.793 18.929 16.886 19.398 18.717 25.021]\n",
      "Better loss, from 428273.2755260551 to 427925.315287353 with particle [17.305 17.402 17.46  17.125 14.176 16.764 16.33  22.593]\n",
      "Better loss, from 427925.315287353 to 427076.565494431 with particle [17.757 16.281 17.017 15.374 14.425 15.875 16.264 22.534]\n",
      "Better loss, from 427076.565494431 to 426481.04385013436 with particle [20.892 21.438 20.356 18.813 17.391 19.57  19.61  25.803]\n",
      "Better loss, from 426481.04385013436 to 422298.32295169 with particle [20.748 17.463 19.543 16.895 16.34  17.659 18.354 26.274]\n",
      "Iteration 6, loss: 422298.32295169, particle: [20.748 17.463 19.543 16.895 16.34  17.659 18.354 26.274]\n",
      "Better loss, from 422298.32295169 to 420655.5056328389 with particle [27.501 23.025 26.019 22.259 21.679 23.28  24.488 34.884]\n",
      "Better loss, from 420655.5056328389 to 416602.9491908678 with particle [31.5   23.475 28.833 23.568 23.855 24.984 27.148 39.567]\n",
      "Better loss, from 416602.9491908678 to 408176.8180824013 with particle [37.645 23.296 33.564 26.289 26.892 27.669 31.195 47.068]\n",
      "Better loss, from 408176.8180824013 to 406145.38103287603 with particle [46.453 25.403 40.506 30.765 32.028 32.457 37.502 57.774]\n",
      "Better loss, from 406145.38103287603 to 403686.84745547106 with particle [49.063 25.453 42.103 31.388 33.488 33.48  39.255 60.72 ]\n",
      "Better loss, from 403686.84745547106 to 392845.6606941555 with particle [53.764 24.107 44.964 32.341 35.449 34.762 41.92  66.068]\n",
      "Better loss, from 392845.6606941555 to 368170.6993616959 with particle [59.066 19.118 47.688 31.936 36.902 34.697 44.066 72.797]\n",
      "Better loss, from 368170.6993616959 to 351232.89155168604 with particle [80.568 20.714 63.569 40.749 48.584 44.61  58.566 98.861]\n",
      "Better loss, from 351232.89155168604 to 345440.3560529264 with particle [75.594 18.2   58.955 37.369 45.426 41.013 54.045 93.078]\n",
      "Iteration 7, loss: 345440.3560529264, particle: [75.594 18.2   58.955 37.369 45.426 41.013 54.045 93.078]\n",
      "Better loss, from 345440.3560529264 to 314221.9279360185 with particle [ 91.11   14.151  68.652  40.596  52.112  45.02   62.661 111.626]\n",
      "Better loss, from 314221.9279360185 to 261794.67430454676 with particle [121.384   6.824  88.017  47.387  65.509  53.68   79.764 147.757]\n",
      "Better loss, from 261794.67430454676 to 255802.21442221684 with particle [153.544  12.057 112.348  61.774  83.895  69.649 101.923 186.882]\n",
      "Better loss, from 255802.21442221684 to 249537.9328089563 with particle [182.635  11.991 132.879  72.02   99.057  81.504 120.574 222.023]\n",
      "Better loss, from 249537.9328089563 to 239422.79792424105 with particle [259.397   7.474 186.56   97.475 137.9   111.081 168.716 315.393]\n",
      "Iteration 8, loss: 239422.79792424105, particle: [259.397   7.474 186.56   97.475 137.9   111.081 168.716 315.393]\n",
      "Better loss, from 239422.79792424105 to 207607.79685394946 with particle [205.17    6.512 147.264  76.875 109.22   87.436 133.595 249.527]\n",
      "Better loss, from 207607.79685394946 to 186466.28946453577 with particle [260.69    1.989 185.269  94.214 136.806 107.935 167.785 316.626]\n",
      "Better loss, from 186466.28946453577 to 175418.63138775007 with particle [265.046   0.271 188.162  95.116 138.402 109.12  170.022 321.737]\n",
      "Iteration 9, loss: 175418.63138775007, particle: [265.046   0.271 188.162  95.116 138.402 109.12  170.022 321.737]\n",
      "Iteration 10, loss: 175418.63138775007, particle: [265.046   0.271 188.162  95.116 138.402 109.12  170.022 321.737]\n",
      "Iteration 11, loss: 175418.63138775007, particle: [265.046   0.271 188.162  95.116 138.402 109.12  170.022 321.737]\n",
      "Better loss, from 175418.63138775007 to 172061.7269133967 with particle [241.051  -1.525 170.46   85.44  125.094  98.065 153.934 292.571]\n",
      "Iteration 12, loss: 172061.7269133967, particle: [241.051  -1.525 170.46   85.44  125.094  98.065 153.934 292.571]\n",
      "Iteration 13, loss: 172061.7269133967, particle: [241.051  -1.525 170.46   85.44  125.094  98.065 153.934 292.571]\n",
      "Iteration 14, loss: 172061.7269133967, particle: [241.051  -1.525 170.46   85.44  125.094  98.065 153.934 292.571]\n",
      "Better loss, from 172061.7269133967 to 169588.93110614098 with particle [227.967   0.359 161.629  81.677 118.886  93.499 146.148 276.845]\n",
      "Iteration 15, loss: 169588.93110614098, particle: [227.967   0.359 161.629  81.677 118.886  93.499 146.148 276.845]\n",
      "Iteration 16, loss: 169588.93110614098, particle: [227.967   0.359 161.629  81.677 118.886  93.499 146.148 276.845]\n",
      "Better loss, from 169588.93110614098 to 168163.18281838938 with particle [245.089  -0.114 173.59   87.434 127.762 100.162 157.013 297.612]\n",
      "Iteration 17, loss: 168163.18281838938, particle: [245.089  -0.114 173.59   87.434 127.762 100.162 157.013 297.612]\n",
      "Iteration 18, loss: 168163.18281838938, particle: [245.089  -0.114 173.59   87.434 127.762 100.162 157.013 297.612]\n",
      "Better loss, from 168163.18281838938 to 168144.5374089794 with particle [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 19, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 20, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 21, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 22, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 23, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n",
      "Iteration 24, loss: 168144.5374089794, particle: [246.638  -0.373 174.586  87.786 128.499 100.612 157.898 299.47 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([246.638,  -0.373, 174.586,  87.786, 128.499, 100.612, 157.898,\n       299.47 ])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pso_instance = PSO(func=loss_all_implicit, init_pos=test_run_weight, n_particles=25)\n",
    "res_s, last_loss, hist_loss = pso_instance.optimize()\n",
    "res_s\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history, label='loss')\n",
    "  plt.xlabel('Epoch')\n",
    "\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "plot_loss(hist_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss: 166536.95971584925\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final test loss: {loss_all(res_s, test_features, test_labels)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}